{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rCpfPbqwIuV"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p32Cj8zwNnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b4415f9-035f-4594-efa7-bcc8f0b478fc"
      },
      "source": [
        "imdb=keras.datasets.imdb\n",
        "vocabulary_size = 1000\n",
        "Max_len = 500\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=vocabulary_size)\n",
        "(train_sequences_removed, train_labels_removed), (test_sequences_removed, test_labels_removed) = tf.keras.datasets.imdb.load_data(num_words=vocabulary_size, maxlen=(Max_len+1))\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZyStouQfkTA",
        "outputId": "54e58b4b-c3b0-4850-94b9-e51e72d7f477"
      },
      "source": [
        "print (train_sequences[6]) # numbers\n",
        "print(test_sequences[6]) #label"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 365, 2, 5, 2, 354, 11, 14, 2, 2, 7, 2, 2, 2, 356, 44, 4, 2, 500, 746, 5, 200, 4, 2, 11, 2, 2, 2, 2, 2, 5, 2, 26, 6, 2, 2, 17, 369, 37, 215, 2, 143, 2, 5, 2, 8, 2, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 2, 4, 2, 7, 4, 2, 2, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2, 5, 2, 81, 55, 52, 2]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0BnIDzbf1WG",
        "outputId": "7670b650-62df-4f9d-87a2-eafd8b546f6d"
      },
      "source": [
        "word_ind = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word_ind.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in train_sequences[6]])\n",
        "print('---label---')\n",
        "print(train_labels[6])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---review with words---\n",
            "['the', 'and', 'full', 'and', 'to', 'and', 'boring', 'this', 'as', 'and', 'and', 'br', 'and', 'and', 'and', 'need', 'has', 'of', 'and', 'b', 'message', 'to', 'may', 'of', 'and', 'this', 'and', 'and', 'and', 'and', 'and', 'to', 'and', 'he', 'is', 'and', 'and', 'movie', 'women', 'like', \"isn't\", 'and', \"i'm\", 'and', 'to', 'and', 'in', 'and', 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'and', 'of', 'and', 'br', 'of', 'and', 'and', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'and', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'and', 'to', 'and', 'people', 'time', 'very', 'and']\n",
            "---label---\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWQcOEWmgrCZ"
      },
      "source": [
        "#def one_hot_encode(data): \n",
        " # for i in X_train:\n",
        "  #  data = tf.one_hot(indices=i,depth=vocabulary_size)\n",
        "    \n",
        "  #return data;"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1jlB_HUiM8o",
        "outputId": "0180ff5a-8d12-4898-dfda-be61874c50ce"
      },
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "min_len = min(sequence_lengths)\n",
        "print(len(sequence_lengths))\n",
        "print(max_len)\n",
        "print(min_len)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "2494\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7u8C_EGq04v",
        "outputId": "06599dd4-cf02-44ce-b5f6-3de88032c8d5"
      },
      "source": [
        "seq_len = 200\n",
        "train_sequences_removed_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences_removed, maxlen=Max_len)\n",
        "test_sequences_removed_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences_removed, maxlen=Max_len)\n",
        "\n",
        "train_data_removed = tf.data.Dataset.from_tensor_slices((train_sequences_removed_padded, train_labels_removed.astype(np.float32))).batch(batch_size, drop_remainder = True).repeat()\n",
        "test_data_removed = tf.data.Dataset.from_tensor_slices((train_sequences_removed_padded, train_labels_removed.astype(np.float32))).batch(batch_size, drop_remainder=True)\n",
        "\n",
        "sequence_lengths_removed = [len(sequence) for sequence in train_sequences_removed_padded]\n",
        "max_len_rem = max(sequence_lengths_removed)\n",
        "min_len_rem = min(sequence_lengths_removed)\n",
        "print(len(sequence_lengths_removed))\n",
        "print(max_len_rem)\n",
        "print(min_len_rem)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22892\n",
            "500\n",
            "500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH8QkmmEuRnY"
      },
      "source": [
        "optimizer = tf.optimizers.Adam()\n",
        "\n",
        "loss_fn = tf.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "train_acc_metric = tf.metrics.BinaryAccuracy\n",
        "\n",
        "training_steps = 500"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pazDSdfDQO-J"
      },
      "source": [
        "def rnn_model(sequences):\n",
        "  old_state = tf.zeros((128, 50))\n",
        "  for step in range(500):\n",
        "    x_t = sequences[:, step]\n",
        "    x_t = tf.one_hot(x_t, depth = vocabulary_size)\n",
        "    a_t = tf.math.add(bias, tf.matmul(old_state, W), tf.matmul(x_t, U))\n",
        "    new_state = tf.math.tanh(a_t)\n",
        "    old_state = new_state\n",
        "\n",
        "  out = output_layer(old_state)\n",
        "  return out\n",
        "\n",
        "def output_layer(out):\n",
        "  o_t =tf.math.add(out_bias, tf.matmul(out, V)) \n",
        "  op = tf.keras.activations.sigmoid(o_t)\n",
        "\n",
        "  return op"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXmQBB2JR4GU"
      },
      "source": [
        "def train_step( sequences, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = rnn_model(sequences)\n",
        "        xent = loss_fn(labels, logits)\n",
        "\n",
        "    variables = [U, W, V, bias, out_bias]\n",
        "    gradients = tape.gradient(xent, variables)\n",
        "    optimizer.apply_gradients((gradients,variables) for(gradients, variables) in zip(gradients, variables) if gradients is not None)\n",
        "\n",
        "    return xent, logits"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KpAKWNPSXPo",
        "outputId": "58871eaa-c675-4fff-fab9-dfc7735642a3"
      },
      "source": [
        "U = tf.Variable(np.random.uniform(low=-0.1, high=0.1, size=[1000, 50]).astype(np.float32), name=\"U\")\n",
        "W = tf.Variable(np.random.uniform(low=-0.1, high=0.1, size=[50, 50]).astype(np.float32), name=\"W\")\n",
        "bias = tf.Variable(np.random.uniform(low=-0.1, high=0.1, size=[128, 50]).astype(np.float32), name=\"bias\")\n",
        "V = tf.Variable(np.random.uniform(low=-0.1, high=0.1, size=[50, 1]).astype(np.float32), name=\"V\")\n",
        "out_bias = tf.Variable(np.random.uniform(low=-0.1, high=0.1, size=[128, 1]).astype(np.float32), name=\"out_bias\")\n",
        "for step, (sequence_batch, label_batch) in enumerate (train_data_removed):\n",
        "    if step > training_steps:\n",
        "        break\n",
        "    xent, logits = train_step(sequence_batch, label_batch)\n",
        "    if not step % 100:\n",
        "        train_acc_metric = tf.metrics.binary_accuracy(label_batch, logits)\n",
        "        acc = tf.math.reduce_mean(train_acc_metric)\n",
        "        print(\" Loss: {} Accuracy: {}\".format(step, xent, acc))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loss: 0 Accuracy: 0.7628380060195923\n",
            " Loss: 100 Accuracy: 0.7036460638046265\n",
            " Loss: 200 Accuracy: 0.7054107785224915\n",
            " Loss: 300 Accuracy: 0.6909956336021423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyS3uPyeeC0x"
      },
      "source": [
        "for sequence_batch, label_batch in test_data_removed:\n",
        "\n",
        "    xent, logits = train_step(sequence_batch, label_batch)\n",
        "    train_acc_metric = tf.metrics.binary_accuracy(label_batch, logits)\n",
        "    acc = tf.math.reduce_mean(train_acc_metric)\n",
        "    print(\" Loss: {} Accuracy: {}\".format(step, xent, acc))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG1eo5lMf3x6"
      },
      "source": [
        "1) Food for thought #1: Why is this wasteful? \n",
        "\n",
        "Important information might be eliminated due to elimination of longer sequence.\n",
        "smarter way-\n",
        "\n",
        "2) Food for thought #2: Between truncating long sequences and removing them, which option do you think is better? Why?\n",
        "\n",
        "Removing might be better.Because with truncate we truncate the entire sequence,\n",
        "with remove we remove only a part.\n",
        "\n",
        "\n",
        "3)Food for thought #3: Can you think of a way to avoid the one-hot vectors completely? Even if you cannot implement it, a conceptual idea is fine.\n",
        "\n",
        "Dummy encoding- uses N-1 features to represent N labels/categories.\n",
        "LabelEncoder\n",
        "\n",
        "5)Food for thought #5: All sequences start with the same special “beginning of sequence” token (coded by index 1). Given this fact, is there a point in learning an initial state? Why (not)?\n",
        "\n",
        "Yes we need to learn. As it contributes in the next state calculation.\n",
        "\n",
        "6)pad_sequences allows for pre or post padding. Try both to see the difference. Which option do you think is better?\n",
        "\n",
        "Pre-padding is better. \n",
        "- Noises can be less\n",
        "- Due to post padding of zeros the final output might get flushed out."
      ]
    }
  ]
}