{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebrJpxDHYw_y"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "189KBAm-Y2P9",
        "outputId": "9641b11c-1181-4837-cda0-ef3ac6183b85"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "from datasets import MNISTDataset"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a703cfe8-5315-4384-8b29-911e25b44042\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a703cfe8-5315-4384-8b29-911e25b44042\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving datasets.py to datasets (3).py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2uM5RXZUZAu"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYO41moyY8ws"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLj4XBf3ZgWW"
      },
      "source": [
        "train_images = (train_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "test_images = (test_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "\n",
        "train_labels = train_labels.astype(np.int32)\n",
        "test_labels = test_labels.astype(np.int32)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_data = train_data.batch(128)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2udfePtZxV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "891a760c-addc-4a22-a16f-ae59004ea135"
      },
      "source": [
        "train_steps = 1000\n",
        "learning_rate = 0.1\n",
        "\n",
        "W_h=tf.Variable(tf.random.normal([784,60],mean = 0,stddev=0.5,dtype=tf.dtypes.float32))\n",
        "b_h= tf.Variable(tf.random.normal( [60],mean = 0,stddev=0.5 ,dtype=tf.dtypes.float32))\n",
        "W_o=tf.Variable(tf.random.normal([60,10],mean = 0,stddev=0.5 ,dtype=tf.dtypes.float32))\n",
        "b_o= tf.Variable(tf.random.normal( [10], mean = 0,stddev=0.5,dtype=tf.dtypes.float32))\n",
        "\n",
        "for step, (image_batch, label_batch) in enumerate(train_data):\n",
        "  if step>train_steps:\n",
        "    break\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "     logits_1 = tf.nn.relu(tf.matmul(image_batch, W_h) + b_h)\n",
        "     logits_2 = (tf.matmul(logits_1, W_o) + b_o)\n",
        "     xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits_2, labels=label_batch))\n",
        "     \n",
        "     \n",
        "  \n",
        "     grads = tape.gradient(xent, [W_o,b_o,W_h,b_h])\n",
        "     W_o.assign_sub(learning_rate * grads[0])\n",
        "     b_o.assign_sub(learning_rate * grads[1])\n",
        "     W_h.assign_sub(learning_rate * grads[2])\n",
        "     b_h.assign_sub(learning_rate * grads[3])\n",
        "\n",
        "  if step % 100:\n",
        "      print(step)\n",
        "      preds = tf.argmax(logits_2, axis=1, output_type=tf.int32)\n",
        "      acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),\n",
        "                             tf.float32))\n",
        "      print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "     \n",
        "\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Loss: 14.289302825927734 Accuracy: 0.109375\n",
            "2\n",
            "Loss: 11.303542137145996 Accuracy: 0.1875\n",
            "3\n",
            "Loss: 9.034351348876953 Accuracy: 0.1171875\n",
            "4\n",
            "Loss: 8.489034652709961 Accuracy: 0.1875\n",
            "5\n",
            "Loss: 6.509423732757568 Accuracy: 0.2265625\n",
            "6\n",
            "Loss: 5.273309707641602 Accuracy: 0.2734375\n",
            "7\n",
            "Loss: 4.980221271514893 Accuracy: 0.2265625\n",
            "8\n",
            "Loss: 5.2599592208862305 Accuracy: 0.234375\n",
            "9\n",
            "Loss: 4.845675945281982 Accuracy: 0.3125\n",
            "10\n",
            "Loss: 4.852930068969727 Accuracy: 0.328125\n",
            "11\n",
            "Loss: 4.058111667633057 Accuracy: 0.390625\n",
            "12\n",
            "Loss: 3.407993793487549 Accuracy: 0.453125\n",
            "13\n",
            "Loss: 2.934558868408203 Accuracy: 0.5390625\n",
            "14\n",
            "Loss: 3.410280704498291 Accuracy: 0.5\n",
            "15\n",
            "Loss: 3.240185022354126 Accuracy: 0.53125\n",
            "16\n",
            "Loss: 2.647278070449829 Accuracy: 0.5703125\n",
            "17\n",
            "Loss: 2.46423602104187 Accuracy: 0.5234375\n",
            "18\n",
            "Loss: 3.065714120864868 Accuracy: 0.4921875\n",
            "19\n",
            "Loss: 2.308990955352783 Accuracy: 0.5625\n",
            "20\n",
            "Loss: 2.364504098892212 Accuracy: 0.640625\n",
            "21\n",
            "Loss: 3.0741865634918213 Accuracy: 0.5234375\n",
            "22\n",
            "Loss: 2.94417667388916 Accuracy: 0.4609375\n",
            "23\n",
            "Loss: 2.97633695602417 Accuracy: 0.5\n",
            "24\n",
            "Loss: 2.2649004459381104 Accuracy: 0.5078125\n",
            "25\n",
            "Loss: 2.27559757232666 Accuracy: 0.578125\n",
            "26\n",
            "Loss: 2.2495014667510986 Accuracy: 0.59375\n",
            "27\n",
            "Loss: 2.146824359893799 Accuracy: 0.53125\n",
            "28\n",
            "Loss: 1.8915966749191284 Accuracy: 0.609375\n",
            "29\n",
            "Loss: 2.4313364028930664 Accuracy: 0.59375\n",
            "30\n",
            "Loss: 1.9930261373519897 Accuracy: 0.6171875\n",
            "31\n",
            "Loss: 2.0066866874694824 Accuracy: 0.6015625\n",
            "32\n",
            "Loss: 2.0047855377197266 Accuracy: 0.625\n",
            "33\n",
            "Loss: 1.9033634662628174 Accuracy: 0.65625\n",
            "34\n",
            "Loss: 1.9926271438598633 Accuracy: 0.5859375\n",
            "35\n",
            "Loss: 1.6108027696609497 Accuracy: 0.671875\n",
            "36\n",
            "Loss: 2.328263759613037 Accuracy: 0.578125\n",
            "37\n",
            "Loss: 2.7186431884765625 Accuracy: 0.546875\n",
            "38\n",
            "Loss: 2.249454975128174 Accuracy: 0.6015625\n",
            "39\n",
            "Loss: 1.0439234972000122 Accuracy: 0.7578125\n",
            "40\n",
            "Loss: 1.5033413171768188 Accuracy: 0.703125\n",
            "41\n",
            "Loss: 1.909657597541809 Accuracy: 0.6640625\n",
            "42\n",
            "Loss: 1.2088271379470825 Accuracy: 0.734375\n",
            "43\n",
            "Loss: 1.5620627403259277 Accuracy: 0.6796875\n",
            "44\n",
            "Loss: 1.4883636236190796 Accuracy: 0.6484375\n",
            "45\n",
            "Loss: 1.4275187253952026 Accuracy: 0.6328125\n",
            "46\n",
            "Loss: 1.1772708892822266 Accuracy: 0.7734375\n",
            "47\n",
            "Loss: 1.0658377408981323 Accuracy: 0.765625\n",
            "48\n",
            "Loss: 1.0067625045776367 Accuracy: 0.7421875\n",
            "49\n",
            "Loss: 0.9741054773330688 Accuracy: 0.7109375\n",
            "50\n",
            "Loss: 1.628830909729004 Accuracy: 0.703125\n",
            "51\n",
            "Loss: 1.2181047201156616 Accuracy: 0.78125\n",
            "52\n",
            "Loss: 1.3321921825408936 Accuracy: 0.6875\n",
            "53\n",
            "Loss: 1.851639986038208 Accuracy: 0.65625\n",
            "54\n",
            "Loss: 1.3754141330718994 Accuracy: 0.65625\n",
            "55\n",
            "Loss: 1.6806538105010986 Accuracy: 0.703125\n",
            "56\n",
            "Loss: 1.6890995502471924 Accuracy: 0.59375\n",
            "57\n",
            "Loss: 1.7554492950439453 Accuracy: 0.609375\n",
            "58\n",
            "Loss: 1.6611905097961426 Accuracy: 0.671875\n",
            "59\n",
            "Loss: 1.384810447692871 Accuracy: 0.640625\n",
            "60\n",
            "Loss: 2.1329479217529297 Accuracy: 0.5390625\n",
            "61\n",
            "Loss: 1.5419427156448364 Accuracy: 0.6953125\n",
            "62\n",
            "Loss: 1.3660215139389038 Accuracy: 0.7265625\n",
            "63\n",
            "Loss: 1.0336025953292847 Accuracy: 0.703125\n",
            "64\n",
            "Loss: 1.565458059310913 Accuracy: 0.6640625\n",
            "65\n",
            "Loss: 1.2105531692504883 Accuracy: 0.6953125\n",
            "66\n",
            "Loss: 1.126430869102478 Accuracy: 0.7265625\n",
            "67\n",
            "Loss: 1.6603803634643555 Accuracy: 0.6796875\n",
            "68\n",
            "Loss: 2.1210484504699707 Accuracy: 0.65625\n",
            "69\n",
            "Loss: 1.9889888763427734 Accuracy: 0.578125\n",
            "70\n",
            "Loss: 0.8181325793266296 Accuracy: 0.7578125\n",
            "71\n",
            "Loss: 1.4247329235076904 Accuracy: 0.7109375\n",
            "72\n",
            "Loss: 1.2559257745742798 Accuracy: 0.6875\n",
            "73\n",
            "Loss: 1.1042065620422363 Accuracy: 0.7578125\n",
            "74\n",
            "Loss: 1.186206579208374 Accuracy: 0.7109375\n",
            "75\n",
            "Loss: 0.9581255912780762 Accuracy: 0.796875\n",
            "76\n",
            "Loss: 0.8550335764884949 Accuracy: 0.7734375\n",
            "77\n",
            "Loss: 0.8989672660827637 Accuracy: 0.8046875\n",
            "78\n",
            "Loss: 1.0658077001571655 Accuracy: 0.71875\n",
            "79\n",
            "Loss: 1.1335828304290771 Accuracy: 0.734375\n",
            "80\n",
            "Loss: 1.0351096391677856 Accuracy: 0.71875\n",
            "81\n",
            "Loss: 0.612939715385437 Accuracy: 0.8203125\n",
            "82\n",
            "Loss: 0.7537919282913208 Accuracy: 0.7578125\n",
            "83\n",
            "Loss: 0.674138605594635 Accuracy: 0.78125\n",
            "84\n",
            "Loss: 0.6066504716873169 Accuracy: 0.828125\n",
            "85\n",
            "Loss: 0.8850967288017273 Accuracy: 0.765625\n",
            "86\n",
            "Loss: 0.9882299900054932 Accuracy: 0.78125\n",
            "87\n",
            "Loss: 1.232813835144043 Accuracy: 0.765625\n",
            "88\n",
            "Loss: 1.0022313594818115 Accuracy: 0.7578125\n",
            "89\n",
            "Loss: 0.8163081407546997 Accuracy: 0.78125\n",
            "90\n",
            "Loss: 1.2103761434555054 Accuracy: 0.7265625\n",
            "91\n",
            "Loss: 1.5226523876190186 Accuracy: 0.671875\n",
            "92\n",
            "Loss: 0.8227999210357666 Accuracy: 0.7734375\n",
            "93\n",
            "Loss: 0.8289695978164673 Accuracy: 0.8046875\n",
            "94\n",
            "Loss: 0.7401275038719177 Accuracy: 0.796875\n",
            "95\n",
            "Loss: 0.9764108061790466 Accuracy: 0.71875\n",
            "96\n",
            "Loss: 1.1511669158935547 Accuracy: 0.7578125\n",
            "97\n",
            "Loss: 1.4552273750305176 Accuracy: 0.703125\n",
            "98\n",
            "Loss: 1.4231940507888794 Accuracy: 0.7265625\n",
            "99\n",
            "Loss: 1.2050975561141968 Accuracy: 0.6875\n",
            "101\n",
            "Loss: 1.2001196146011353 Accuracy: 0.7109375\n",
            "102\n",
            "Loss: 1.0504714250564575 Accuracy: 0.7578125\n",
            "103\n",
            "Loss: 0.7295142412185669 Accuracy: 0.8125\n",
            "104\n",
            "Loss: 0.8274912238121033 Accuracy: 0.8046875\n",
            "105\n",
            "Loss: 0.7422823309898376 Accuracy: 0.8515625\n",
            "106\n",
            "Loss: 0.8608517646789551 Accuracy: 0.8046875\n",
            "107\n",
            "Loss: 1.1351689100265503 Accuracy: 0.765625\n",
            "108\n",
            "Loss: 0.9300522804260254 Accuracy: 0.734375\n",
            "109\n",
            "Loss: 1.0918517112731934 Accuracy: 0.6796875\n",
            "110\n",
            "Loss: 1.0677894353866577 Accuracy: 0.7265625\n",
            "111\n",
            "Loss: 1.4618020057678223 Accuracy: 0.7109375\n",
            "112\n",
            "Loss: 1.3135381937026978 Accuracy: 0.71875\n",
            "113\n",
            "Loss: 1.2115064859390259 Accuracy: 0.703125\n",
            "114\n",
            "Loss: 1.4038151502609253 Accuracy: 0.640625\n",
            "115\n",
            "Loss: 1.8403362035751343 Accuracy: 0.625\n",
            "116\n",
            "Loss: 0.7835023999214172 Accuracy: 0.796875\n",
            "117\n",
            "Loss: 0.7025164365768433 Accuracy: 0.796875\n",
            "118\n",
            "Loss: 1.1910403966903687 Accuracy: 0.7265625\n",
            "119\n",
            "Loss: 0.692966103553772 Accuracy: 0.8203125\n",
            "120\n",
            "Loss: 0.6579815149307251 Accuracy: 0.8125\n",
            "121\n",
            "Loss: 0.9248939752578735 Accuracy: 0.7734375\n",
            "122\n",
            "Loss: 0.921045184135437 Accuracy: 0.7421875\n",
            "123\n",
            "Loss: 1.295823097229004 Accuracy: 0.6875\n",
            "124\n",
            "Loss: 1.0524901151657104 Accuracy: 0.7265625\n",
            "125\n",
            "Loss: 1.0433062314987183 Accuracy: 0.734375\n",
            "126\n",
            "Loss: 0.5086137056350708 Accuracy: 0.84375\n",
            "127\n",
            "Loss: 0.8899593353271484 Accuracy: 0.8125\n",
            "128\n",
            "Loss: 0.648960292339325 Accuracy: 0.8359375\n",
            "129\n",
            "Loss: 1.0317097902297974 Accuracy: 0.7890625\n",
            "130\n",
            "Loss: 1.0473626852035522 Accuracy: 0.75\n",
            "131\n",
            "Loss: 1.4465442895889282 Accuracy: 0.6796875\n",
            "132\n",
            "Loss: 1.1655319929122925 Accuracy: 0.7265625\n",
            "133\n",
            "Loss: 1.0728150606155396 Accuracy: 0.75\n",
            "134\n",
            "Loss: 1.156693458557129 Accuracy: 0.765625\n",
            "135\n",
            "Loss: 0.611910343170166 Accuracy: 0.8203125\n",
            "136\n",
            "Loss: 1.076866865158081 Accuracy: 0.734375\n",
            "137\n",
            "Loss: 1.2605568170547485 Accuracy: 0.6796875\n",
            "138\n",
            "Loss: 0.993831992149353 Accuracy: 0.71875\n",
            "139\n",
            "Loss: 0.8909008502960205 Accuracy: 0.71875\n",
            "140\n",
            "Loss: 0.6250420808792114 Accuracy: 0.8203125\n",
            "141\n",
            "Loss: 0.6759512424468994 Accuracy: 0.8203125\n",
            "142\n",
            "Loss: 0.6095342636108398 Accuracy: 0.84375\n",
            "143\n",
            "Loss: 0.7754291296005249 Accuracy: 0.796875\n",
            "144\n",
            "Loss: 0.6728740930557251 Accuracy: 0.828125\n",
            "145\n",
            "Loss: 1.0837457180023193 Accuracy: 0.7890625\n",
            "146\n",
            "Loss: 0.7498658895492554 Accuracy: 0.8046875\n",
            "147\n",
            "Loss: 0.8895741701126099 Accuracy: 0.75\n",
            "148\n",
            "Loss: 0.7049574851989746 Accuracy: 0.8203125\n",
            "149\n",
            "Loss: 0.8804303407669067 Accuracy: 0.7578125\n",
            "150\n",
            "Loss: 0.7607733011245728 Accuracy: 0.828125\n",
            "151\n",
            "Loss: 0.9971824884414673 Accuracy: 0.765625\n",
            "152\n",
            "Loss: 0.7624755501747131 Accuracy: 0.796875\n",
            "153\n",
            "Loss: 0.6919189691543579 Accuracy: 0.8046875\n",
            "154\n",
            "Loss: 0.5846188068389893 Accuracy: 0.8828125\n",
            "155\n",
            "Loss: 0.5984755754470825 Accuracy: 0.84375\n",
            "156\n",
            "Loss: 0.8455660939216614 Accuracy: 0.7421875\n",
            "157\n",
            "Loss: 0.9410766363143921 Accuracy: 0.765625\n",
            "158\n",
            "Loss: 0.7318868041038513 Accuracy: 0.8125\n",
            "159\n",
            "Loss: 0.5220777988433838 Accuracy: 0.8828125\n",
            "160\n",
            "Loss: 0.550845742225647 Accuracy: 0.859375\n",
            "161\n",
            "Loss: 0.8811438083648682 Accuracy: 0.8125\n",
            "162\n",
            "Loss: 0.8445957899093628 Accuracy: 0.828125\n",
            "163\n",
            "Loss: 1.1818231344223022 Accuracy: 0.7421875\n",
            "164\n",
            "Loss: 0.8427144885063171 Accuracy: 0.796875\n",
            "165\n",
            "Loss: 0.568047285079956 Accuracy: 0.828125\n",
            "166\n",
            "Loss: 0.6487888097763062 Accuracy: 0.78125\n",
            "167\n",
            "Loss: 0.7605574131011963 Accuracy: 0.8046875\n",
            "168\n",
            "Loss: 0.7585914134979248 Accuracy: 0.7890625\n",
            "169\n",
            "Loss: 0.8383631110191345 Accuracy: 0.796875\n",
            "170\n",
            "Loss: 0.4717326760292053 Accuracy: 0.890625\n",
            "171\n",
            "Loss: 0.6737244725227356 Accuracy: 0.8671875\n",
            "172\n",
            "Loss: 0.7630274295806885 Accuracy: 0.7578125\n",
            "173\n",
            "Loss: 1.1243433952331543 Accuracy: 0.734375\n",
            "174\n",
            "Loss: 0.6987001299858093 Accuracy: 0.828125\n",
            "175\n",
            "Loss: 1.221668004989624 Accuracy: 0.6953125\n",
            "176\n",
            "Loss: 1.56770920753479 Accuracy: 0.7265625\n",
            "177\n",
            "Loss: 0.8400900959968567 Accuracy: 0.796875\n",
            "178\n",
            "Loss: 0.735352635383606 Accuracy: 0.8046875\n",
            "179\n",
            "Loss: 0.5825783610343933 Accuracy: 0.8359375\n",
            "180\n",
            "Loss: 0.9027870893478394 Accuracy: 0.796875\n",
            "181\n",
            "Loss: 0.7248618602752686 Accuracy: 0.828125\n",
            "182\n",
            "Loss: 0.5930215716362 Accuracy: 0.8359375\n",
            "183\n",
            "Loss: 0.5585348606109619 Accuracy: 0.8125\n",
            "184\n",
            "Loss: 0.6810577511787415 Accuracy: 0.8203125\n",
            "185\n",
            "Loss: 0.9359721541404724 Accuracy: 0.7890625\n",
            "186\n",
            "Loss: 0.6415438652038574 Accuracy: 0.7734375\n",
            "187\n",
            "Loss: 0.7126592993736267 Accuracy: 0.828125\n",
            "188\n",
            "Loss: 0.4571993052959442 Accuracy: 0.8359375\n",
            "189\n",
            "Loss: 0.7177954316139221 Accuracy: 0.796875\n",
            "190\n",
            "Loss: 0.7144608497619629 Accuracy: 0.8203125\n",
            "191\n",
            "Loss: 0.8538302183151245 Accuracy: 0.7890625\n",
            "192\n",
            "Loss: 0.975161075592041 Accuracy: 0.75\n",
            "193\n",
            "Loss: 1.0529463291168213 Accuracy: 0.703125\n",
            "194\n",
            "Loss: 0.7235760688781738 Accuracy: 0.78125\n",
            "195\n",
            "Loss: 0.5348623394966125 Accuracy: 0.828125\n",
            "196\n",
            "Loss: 0.8102592825889587 Accuracy: 0.796875\n",
            "197\n",
            "Loss: 0.7131156921386719 Accuracy: 0.7734375\n",
            "198\n",
            "Loss: 0.4440150260925293 Accuracy: 0.8515625\n",
            "199\n",
            "Loss: 0.6580192446708679 Accuracy: 0.8125\n",
            "201\n",
            "Loss: 0.8179289102554321 Accuracy: 0.7578125\n",
            "202\n",
            "Loss: 0.6148110628128052 Accuracy: 0.796875\n",
            "203\n",
            "Loss: 0.6534130573272705 Accuracy: 0.796875\n",
            "204\n",
            "Loss: 0.5590248107910156 Accuracy: 0.875\n",
            "205\n",
            "Loss: 0.5309192538261414 Accuracy: 0.859375\n",
            "206\n",
            "Loss: 0.9570619463920593 Accuracy: 0.7734375\n",
            "207\n",
            "Loss: 0.7364089488983154 Accuracy: 0.78125\n",
            "208\n",
            "Loss: 1.009323239326477 Accuracy: 0.7734375\n",
            "209\n",
            "Loss: 0.5786021947860718 Accuracy: 0.8125\n",
            "210\n",
            "Loss: 0.7151163816452026 Accuracy: 0.8515625\n",
            "211\n",
            "Loss: 0.9283702373504639 Accuracy: 0.78125\n",
            "212\n",
            "Loss: 1.2691447734832764 Accuracy: 0.75\n",
            "213\n",
            "Loss: 0.5929732918739319 Accuracy: 0.859375\n",
            "214\n",
            "Loss: 1.1439765691757202 Accuracy: 0.75\n",
            "215\n",
            "Loss: 0.6723955869674683 Accuracy: 0.859375\n",
            "216\n",
            "Loss: 0.6090826392173767 Accuracy: 0.8125\n",
            "217\n",
            "Loss: 0.7828439474105835 Accuracy: 0.8046875\n",
            "218\n",
            "Loss: 0.5016030669212341 Accuracy: 0.8671875\n",
            "219\n",
            "Loss: 0.5956422686576843 Accuracy: 0.84375\n",
            "220\n",
            "Loss: 0.6714320778846741 Accuracy: 0.8046875\n",
            "221\n",
            "Loss: 0.8714185357093811 Accuracy: 0.8046875\n",
            "222\n",
            "Loss: 0.5482162833213806 Accuracy: 0.84375\n",
            "223\n",
            "Loss: 0.9461023807525635 Accuracy: 0.796875\n",
            "224\n",
            "Loss: 0.5093284845352173 Accuracy: 0.859375\n",
            "225\n",
            "Loss: 0.5919344425201416 Accuracy: 0.8359375\n",
            "226\n",
            "Loss: 0.5440725684165955 Accuracy: 0.8515625\n",
            "227\n",
            "Loss: 1.2304974794387817 Accuracy: 0.7421875\n",
            "228\n",
            "Loss: 0.9937462210655212 Accuracy: 0.765625\n",
            "229\n",
            "Loss: 0.8691367506980896 Accuracy: 0.8359375\n",
            "230\n",
            "Loss: 0.5238872766494751 Accuracy: 0.8359375\n",
            "231\n",
            "Loss: 0.7022688388824463 Accuracy: 0.765625\n",
            "232\n",
            "Loss: 0.7171249389648438 Accuracy: 0.7890625\n",
            "233\n",
            "Loss: 0.9718080759048462 Accuracy: 0.7265625\n",
            "234\n",
            "Loss: 0.8609868884086609 Accuracy: 0.7890625\n",
            "235\n",
            "Loss: 1.2244770526885986 Accuracy: 0.75\n",
            "236\n",
            "Loss: 0.5785120725631714 Accuracy: 0.84375\n",
            "237\n",
            "Loss: 0.6048658490180969 Accuracy: 0.8125\n",
            "238\n",
            "Loss: 0.8162925839424133 Accuracy: 0.796875\n",
            "239\n",
            "Loss: 0.9595840573310852 Accuracy: 0.7265625\n",
            "240\n",
            "Loss: 0.82000732421875 Accuracy: 0.8046875\n",
            "241\n",
            "Loss: 0.8129442930221558 Accuracy: 0.75\n",
            "242\n",
            "Loss: 0.6881936192512512 Accuracy: 0.8046875\n",
            "243\n",
            "Loss: 0.8949640393257141 Accuracy: 0.796875\n",
            "244\n",
            "Loss: 1.0587584972381592 Accuracy: 0.7109375\n",
            "245\n",
            "Loss: 0.8703981637954712 Accuracy: 0.7265625\n",
            "246\n",
            "Loss: 0.5457053184509277 Accuracy: 0.8359375\n",
            "247\n",
            "Loss: 1.2827268838882446 Accuracy: 0.7421875\n",
            "248\n",
            "Loss: 0.9096586108207703 Accuracy: 0.7265625\n",
            "249\n",
            "Loss: 0.6319375038146973 Accuracy: 0.78125\n",
            "250\n",
            "Loss: 0.7162917256355286 Accuracy: 0.796875\n",
            "251\n",
            "Loss: 0.8375898599624634 Accuracy: 0.8046875\n",
            "252\n",
            "Loss: 1.0903685092926025 Accuracy: 0.75\n",
            "253\n",
            "Loss: 0.9342976212501526 Accuracy: 0.7265625\n",
            "254\n",
            "Loss: 0.4164273142814636 Accuracy: 0.859375\n",
            "255\n",
            "Loss: 0.7253408432006836 Accuracy: 0.78125\n",
            "256\n",
            "Loss: 0.63481605052948 Accuracy: 0.8515625\n",
            "257\n",
            "Loss: 0.4638653099536896 Accuracy: 0.890625\n",
            "258\n",
            "Loss: 0.7663719654083252 Accuracy: 0.78125\n",
            "259\n",
            "Loss: 0.44545280933380127 Accuracy: 0.8984375\n",
            "260\n",
            "Loss: 0.58348548412323 Accuracy: 0.8359375\n",
            "261\n",
            "Loss: 0.607148289680481 Accuracy: 0.8125\n",
            "262\n",
            "Loss: 0.5772594213485718 Accuracy: 0.8515625\n",
            "263\n",
            "Loss: 0.5867271423339844 Accuracy: 0.796875\n",
            "264\n",
            "Loss: 0.3612535297870636 Accuracy: 0.828125\n",
            "265\n",
            "Loss: 0.395243763923645 Accuracy: 0.8828125\n",
            "266\n",
            "Loss: 0.5302746891975403 Accuracy: 0.859375\n",
            "267\n",
            "Loss: 0.4307715892791748 Accuracy: 0.8828125\n",
            "268\n",
            "Loss: 0.5315887928009033 Accuracy: 0.8515625\n",
            "269\n",
            "Loss: 0.6247032880783081 Accuracy: 0.8203125\n",
            "270\n",
            "Loss: 0.7962433695793152 Accuracy: 0.828125\n",
            "271\n",
            "Loss: 0.7265045046806335 Accuracy: 0.78125\n",
            "272\n",
            "Loss: 0.8523145318031311 Accuracy: 0.765625\n",
            "273\n",
            "Loss: 0.4452427327632904 Accuracy: 0.875\n",
            "274\n",
            "Loss: 0.566961407661438 Accuracy: 0.828125\n",
            "275\n",
            "Loss: 0.543125569820404 Accuracy: 0.828125\n",
            "276\n",
            "Loss: 0.581758975982666 Accuracy: 0.8359375\n",
            "277\n",
            "Loss: 0.7705742716789246 Accuracy: 0.8046875\n",
            "278\n",
            "Loss: 0.4963136613368988 Accuracy: 0.859375\n",
            "279\n",
            "Loss: 0.4799512028694153 Accuracy: 0.875\n",
            "280\n",
            "Loss: 0.5684816241264343 Accuracy: 0.796875\n",
            "281\n",
            "Loss: 0.8144448399543762 Accuracy: 0.7421875\n",
            "282\n",
            "Loss: 0.5185962915420532 Accuracy: 0.8515625\n",
            "283\n",
            "Loss: 0.409535676240921 Accuracy: 0.859375\n",
            "284\n",
            "Loss: 0.9760923385620117 Accuracy: 0.8046875\n",
            "285\n",
            "Loss: 0.6482445001602173 Accuracy: 0.8515625\n",
            "286\n",
            "Loss: 0.31065094470977783 Accuracy: 0.9140625\n",
            "287\n",
            "Loss: 0.6381661891937256 Accuracy: 0.8125\n",
            "288\n",
            "Loss: 0.43028494715690613 Accuracy: 0.828125\n",
            "289\n",
            "Loss: 0.7342750430107117 Accuracy: 0.8125\n",
            "290\n",
            "Loss: 0.5100871324539185 Accuracy: 0.8359375\n",
            "291\n",
            "Loss: 0.7570699453353882 Accuracy: 0.8515625\n",
            "292\n",
            "Loss: 0.7545530796051025 Accuracy: 0.796875\n",
            "293\n",
            "Loss: 0.7453545331954956 Accuracy: 0.7890625\n",
            "294\n",
            "Loss: 0.6965782046318054 Accuracy: 0.7421875\n",
            "295\n",
            "Loss: 0.6649048328399658 Accuracy: 0.8515625\n",
            "296\n",
            "Loss: 0.42878881096839905 Accuracy: 0.8828125\n",
            "297\n",
            "Loss: 0.5962710380554199 Accuracy: 0.8046875\n",
            "298\n",
            "Loss: 0.5577222108840942 Accuracy: 0.859375\n",
            "299\n",
            "Loss: 0.4701484143733978 Accuracy: 0.8671875\n",
            "301\n",
            "Loss: 0.5613056421279907 Accuracy: 0.8515625\n",
            "302\n",
            "Loss: 0.5896070003509521 Accuracy: 0.828125\n",
            "303\n",
            "Loss: 0.45917099714279175 Accuracy: 0.84375\n",
            "304\n",
            "Loss: 0.4095172882080078 Accuracy: 0.8671875\n",
            "305\n",
            "Loss: 0.6070179343223572 Accuracy: 0.8046875\n",
            "306\n",
            "Loss: 0.5671826601028442 Accuracy: 0.859375\n",
            "307\n",
            "Loss: 1.1402620077133179 Accuracy: 0.671875\n",
            "308\n",
            "Loss: 0.45702067017555237 Accuracy: 0.8671875\n",
            "309\n",
            "Loss: 0.5885004997253418 Accuracy: 0.8359375\n",
            "310\n",
            "Loss: 0.6433625817298889 Accuracy: 0.765625\n",
            "311\n",
            "Loss: 0.7198148369789124 Accuracy: 0.8125\n",
            "312\n",
            "Loss: 0.6061658263206482 Accuracy: 0.78125\n",
            "313\n",
            "Loss: 0.4158903956413269 Accuracy: 0.875\n",
            "314\n",
            "Loss: 0.49586984515190125 Accuracy: 0.828125\n",
            "315\n",
            "Loss: 0.48675912618637085 Accuracy: 0.859375\n",
            "316\n",
            "Loss: 0.7385700345039368 Accuracy: 0.8125\n",
            "317\n",
            "Loss: 0.5176651477813721 Accuracy: 0.8125\n",
            "318\n",
            "Loss: 0.6541032791137695 Accuracy: 0.8359375\n",
            "319\n",
            "Loss: 0.5298322439193726 Accuracy: 0.8359375\n",
            "320\n",
            "Loss: 0.6465152502059937 Accuracy: 0.8359375\n",
            "321\n",
            "Loss: 0.5009080171585083 Accuracy: 0.828125\n",
            "322\n",
            "Loss: 0.7406890392303467 Accuracy: 0.828125\n",
            "323\n",
            "Loss: 0.8744016885757446 Accuracy: 0.7890625\n",
            "324\n",
            "Loss: 0.6727179884910583 Accuracy: 0.84375\n",
            "325\n",
            "Loss: 0.5090885162353516 Accuracy: 0.8359375\n",
            "326\n",
            "Loss: 0.5722616314888 Accuracy: 0.796875\n",
            "327\n",
            "Loss: 0.5131843090057373 Accuracy: 0.8828125\n",
            "328\n",
            "Loss: 0.4908897280693054 Accuracy: 0.875\n",
            "329\n",
            "Loss: 0.716040313243866 Accuracy: 0.7734375\n",
            "330\n",
            "Loss: 0.7066451907157898 Accuracy: 0.8046875\n",
            "331\n",
            "Loss: 0.880244791507721 Accuracy: 0.78125\n",
            "332\n",
            "Loss: 0.599576473236084 Accuracy: 0.84375\n",
            "333\n",
            "Loss: 0.6571668982505798 Accuracy: 0.828125\n",
            "334\n",
            "Loss: 0.7604861259460449 Accuracy: 0.8046875\n",
            "335\n",
            "Loss: 0.6687527894973755 Accuracy: 0.8046875\n",
            "336\n",
            "Loss: 0.6950607299804688 Accuracy: 0.8046875\n",
            "337\n",
            "Loss: 0.5160341858863831 Accuracy: 0.828125\n",
            "338\n",
            "Loss: 0.2344920039176941 Accuracy: 0.9296875\n",
            "339\n",
            "Loss: 0.48395076394081116 Accuracy: 0.8359375\n",
            "340\n",
            "Loss: 0.41972851753234863 Accuracy: 0.8984375\n",
            "341\n",
            "Loss: 0.4242205023765564 Accuracy: 0.859375\n",
            "342\n",
            "Loss: 0.7571163177490234 Accuracy: 0.828125\n",
            "343\n",
            "Loss: 0.4545542597770691 Accuracy: 0.8203125\n",
            "344\n",
            "Loss: 0.6145200133323669 Accuracy: 0.84375\n",
            "345\n",
            "Loss: 0.6520211100578308 Accuracy: 0.796875\n",
            "346\n",
            "Loss: 0.6694467067718506 Accuracy: 0.8125\n",
            "347\n",
            "Loss: 0.5886660814285278 Accuracy: 0.8515625\n",
            "348\n",
            "Loss: 0.49576088786125183 Accuracy: 0.84375\n",
            "349\n",
            "Loss: 0.5021856427192688 Accuracy: 0.8203125\n",
            "350\n",
            "Loss: 0.6609123945236206 Accuracy: 0.84375\n",
            "351\n",
            "Loss: 0.5922633409500122 Accuracy: 0.828125\n",
            "352\n",
            "Loss: 0.6846182942390442 Accuracy: 0.7890625\n",
            "353\n",
            "Loss: 0.4679041802883148 Accuracy: 0.8671875\n",
            "354\n",
            "Loss: 0.5182183980941772 Accuracy: 0.859375\n",
            "355\n",
            "Loss: 1.0806008577346802 Accuracy: 0.7109375\n",
            "356\n",
            "Loss: 0.5595815181732178 Accuracy: 0.84375\n",
            "357\n",
            "Loss: 0.7966533899307251 Accuracy: 0.7890625\n",
            "358\n",
            "Loss: 0.6942976713180542 Accuracy: 0.78125\n",
            "359\n",
            "Loss: 0.7597328424453735 Accuracy: 0.8125\n",
            "360\n",
            "Loss: 0.8038363456726074 Accuracy: 0.8359375\n",
            "361\n",
            "Loss: 0.7500514984130859 Accuracy: 0.7734375\n",
            "362\n",
            "Loss: 0.7352632880210876 Accuracy: 0.796875\n",
            "363\n",
            "Loss: 0.3593531847000122 Accuracy: 0.8671875\n",
            "364\n",
            "Loss: 0.29298967123031616 Accuracy: 0.890625\n",
            "365\n",
            "Loss: 0.46741360425949097 Accuracy: 0.84375\n",
            "366\n",
            "Loss: 0.47839319705963135 Accuracy: 0.875\n",
            "367\n",
            "Loss: 0.5197181105613708 Accuracy: 0.84375\n",
            "368\n",
            "Loss: 0.5084114670753479 Accuracy: 0.8203125\n",
            "369\n",
            "Loss: 0.8313588500022888 Accuracy: 0.734375\n",
            "370\n",
            "Loss: 0.7568695545196533 Accuracy: 0.765625\n",
            "371\n",
            "Loss: 0.7202508449554443 Accuracy: 0.796875\n",
            "372\n",
            "Loss: 0.47946757078170776 Accuracy: 0.875\n",
            "373\n",
            "Loss: 0.3223521411418915 Accuracy: 0.8984375\n",
            "374\n",
            "Loss: 0.7985127568244934 Accuracy: 0.8125\n",
            "375\n",
            "Loss: 0.39585399627685547 Accuracy: 0.8671875\n",
            "376\n",
            "Loss: 0.3420942723751068 Accuracy: 0.8828125\n",
            "377\n",
            "Loss: 0.6507831811904907 Accuracy: 0.8671875\n",
            "378\n",
            "Loss: 0.30418679118156433 Accuracy: 0.9375\n",
            "379\n",
            "Loss: 0.4600433111190796 Accuracy: 0.828125\n",
            "380\n",
            "Loss: 0.5485839247703552 Accuracy: 0.828125\n",
            "381\n",
            "Loss: 0.5232428908348083 Accuracy: 0.8359375\n",
            "382\n",
            "Loss: 0.9631682634353638 Accuracy: 0.7421875\n",
            "383\n",
            "Loss: 0.9228767156600952 Accuracy: 0.78125\n",
            "384\n",
            "Loss: 0.7527293562889099 Accuracy: 0.7578125\n",
            "385\n",
            "Loss: 0.3815831243991852 Accuracy: 0.890625\n",
            "386\n",
            "Loss: 0.9287666082382202 Accuracy: 0.703125\n",
            "387\n",
            "Loss: 0.9465765357017517 Accuracy: 0.75\n",
            "388\n",
            "Loss: 0.5409117341041565 Accuracy: 0.8046875\n",
            "389\n",
            "Loss: 0.8764228820800781 Accuracy: 0.765625\n",
            "390\n",
            "Loss: 0.6331150531768799 Accuracy: 0.8125\n",
            "391\n",
            "Loss: 0.6964422464370728 Accuracy: 0.8359375\n",
            "392\n",
            "Loss: 0.4452650249004364 Accuracy: 0.8828125\n",
            "393\n",
            "Loss: 1.1885921955108643 Accuracy: 0.765625\n",
            "394\n",
            "Loss: 0.42759090662002563 Accuracy: 0.8515625\n",
            "395\n",
            "Loss: 0.9817981123924255 Accuracy: 0.765625\n",
            "396\n",
            "Loss: 0.5522974729537964 Accuracy: 0.828125\n",
            "397\n",
            "Loss: 0.4688970148563385 Accuracy: 0.8515625\n",
            "398\n",
            "Loss: 0.27653685212135315 Accuracy: 0.8984375\n",
            "399\n",
            "Loss: 0.49529343843460083 Accuracy: 0.796875\n",
            "401\n",
            "Loss: 0.5336319208145142 Accuracy: 0.859375\n",
            "402\n",
            "Loss: 0.451272189617157 Accuracy: 0.890625\n",
            "403\n",
            "Loss: 0.3301404118537903 Accuracy: 0.8828125\n",
            "404\n",
            "Loss: 0.47005364298820496 Accuracy: 0.8828125\n",
            "405\n",
            "Loss: 0.4419953227043152 Accuracy: 0.8671875\n",
            "406\n",
            "Loss: 0.7768458127975464 Accuracy: 0.828125\n",
            "407\n",
            "Loss: 0.9007989168167114 Accuracy: 0.7421875\n",
            "408\n",
            "Loss: 0.644436776638031 Accuracy: 0.84375\n",
            "409\n",
            "Loss: 0.650521993637085 Accuracy: 0.8671875\n",
            "410\n",
            "Loss: 0.25162723660469055 Accuracy: 0.9375\n",
            "411\n",
            "Loss: 0.37438341975212097 Accuracy: 0.8828125\n",
            "412\n",
            "Loss: 0.6013503074645996 Accuracy: 0.8359375\n",
            "413\n",
            "Loss: 0.8641656637191772 Accuracy: 0.796875\n",
            "414\n",
            "Loss: 0.4895493984222412 Accuracy: 0.8046875\n",
            "415\n",
            "Loss: 0.7296028137207031 Accuracy: 0.8515625\n",
            "416\n",
            "Loss: 0.24895057082176208 Accuracy: 0.890625\n",
            "417\n",
            "Loss: 0.39607930183410645 Accuracy: 0.828125\n",
            "418\n",
            "Loss: 0.6619116067886353 Accuracy: 0.8125\n",
            "419\n",
            "Loss: 0.55042964220047 Accuracy: 0.8203125\n",
            "420\n",
            "Loss: 0.6115518808364868 Accuracy: 0.859375\n",
            "421\n",
            "Loss: 0.6597579717636108 Accuracy: 0.828125\n",
            "422\n",
            "Loss: 0.39653658866882324 Accuracy: 0.8671875\n",
            "423\n",
            "Loss: 0.37115776538848877 Accuracy: 0.8984375\n",
            "424\n",
            "Loss: 0.47351911664009094 Accuracy: 0.84375\n",
            "425\n",
            "Loss: 0.3268297016620636 Accuracy: 0.890625\n",
            "426\n",
            "Loss: 0.4353184401988983 Accuracy: 0.859375\n",
            "427\n",
            "Loss: 0.4038142263889313 Accuracy: 0.9140625\n",
            "428\n",
            "Loss: 0.4909370541572571 Accuracy: 0.875\n",
            "429\n",
            "Loss: 0.7234828472137451 Accuracy: 0.7890625\n",
            "430\n",
            "Loss: 0.4064287841320038 Accuracy: 0.875\n",
            "431\n",
            "Loss: 0.5480029582977295 Accuracy: 0.8125\n",
            "432\n",
            "Loss: 0.40951985120773315 Accuracy: 0.8515625\n",
            "433\n",
            "Loss: 0.44128039479255676 Accuracy: 0.859375\n",
            "434\n",
            "Loss: 0.317438006401062 Accuracy: 0.859375\n",
            "435\n",
            "Loss: 0.4197869300842285 Accuracy: 0.921875\n",
            "436\n",
            "Loss: 0.4140002727508545 Accuracy: 0.8984375\n",
            "437\n",
            "Loss: 0.3307063579559326 Accuracy: 0.8828125\n",
            "438\n",
            "Loss: 0.3397565186023712 Accuracy: 0.8671875\n",
            "439\n",
            "Loss: 0.58243727684021 Accuracy: 0.8203125\n",
            "440\n",
            "Loss: 0.3407936692237854 Accuracy: 0.9140625\n",
            "441\n",
            "Loss: 0.6737911701202393 Accuracy: 0.84375\n",
            "442\n",
            "Loss: 0.5774891972541809 Accuracy: 0.78125\n",
            "443\n",
            "Loss: 0.32359135150909424 Accuracy: 0.8828125\n",
            "444\n",
            "Loss: 0.3605923652648926 Accuracy: 0.9140625\n",
            "445\n",
            "Loss: 0.5930008292198181 Accuracy: 0.828125\n",
            "446\n",
            "Loss: 0.34941279888153076 Accuracy: 0.8984375\n",
            "447\n",
            "Loss: 0.5733774900436401 Accuracy: 0.8203125\n",
            "448\n",
            "Loss: 0.31374526023864746 Accuracy: 0.8828125\n",
            "449\n",
            "Loss: 0.469126433134079 Accuracy: 0.8515625\n",
            "450\n",
            "Loss: 0.7488381862640381 Accuracy: 0.8359375\n",
            "451\n",
            "Loss: 0.4413405656814575 Accuracy: 0.859375\n",
            "452\n",
            "Loss: 0.4410640597343445 Accuracy: 0.84375\n",
            "453\n",
            "Loss: 0.498966246843338 Accuracy: 0.859375\n",
            "454\n",
            "Loss: 0.2668114900588989 Accuracy: 0.921875\n",
            "455\n",
            "Loss: 0.28205808997154236 Accuracy: 0.8984375\n",
            "456\n",
            "Loss: 0.3531471788883209 Accuracy: 0.8984375\n",
            "457\n",
            "Loss: 0.39880165457725525 Accuracy: 0.890625\n",
            "458\n",
            "Loss: 0.2708245515823364 Accuracy: 0.9296875\n",
            "459\n",
            "Loss: 0.4300055503845215 Accuracy: 0.8671875\n",
            "460\n",
            "Loss: 0.11715589463710785 Accuracy: 0.96875\n",
            "461\n",
            "Loss: 0.15187260508537292 Accuracy: 0.9453125\n",
            "462\n",
            "Loss: 0.12274471670389175 Accuracy: 0.9609375\n",
            "463\n",
            "Loss: 0.5731966495513916 Accuracy: 0.8359375\n",
            "464\n",
            "Loss: 0.2381078451871872 Accuracy: 0.921875\n",
            "465\n",
            "Loss: 0.1762831062078476 Accuracy: 0.9375\n",
            "466\n",
            "Loss: 0.8331893682479858 Accuracy: 0.828125\n",
            "467\n",
            "Loss: 0.12476921081542969 Accuracy: 0.9609375\n",
            "468\n",
            "Loss: 0.7603440284729004 Accuracy: 0.7604166865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Q3QJ8cKSvH",
        "outputId": "79a25837-6314-4e15-de7b-5a870cdf4d16"
      },
      "source": [
        "test_preds_h = tf.matmul(test_images, W_h) + b_h\n",
        "test_preds = tf.argmax(tf.matmul(test_preds_h, W_o) + b_o, axis=1,\n",
        "                       output_type=tf.int32)                     \n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels),\n",
        "                             tf.float32))\n",
        "print('test accuracy:',acc)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: tf.Tensor(0.5262, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHEf8C1hcXJf"
      },
      "source": [
        "\n",
        "1. data.repeat.shuffle.batch will repeat the data,shuffle the repeated data and divide it into batches.\n",
        "2. data.batch.shuffle.repeat will batch the data, shuffle the batch data and repear the shuffled batches.\n",
        "3. data.shuffle.batch.repeat will shuffle the data, then divide it into batches and repeat the divided batches.\n",
        "4. data.repeat.batch.shuffle will repeat the data, batch the repeated data and then shuffle these repeated batches.\n",
        "5. data.shuffle.repeat.batch will first shuffle the data, then repeat the shuffled data and then divivde it into batches.\n",
        "6. data.batch.repeat.shuffle will batch the data, repeat the batches of data and repeat the shuffled batches.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwbsF4YpcZY_"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VRNZFxXXw6t"
      },
      "source": [
        "Fail2.py\n",
        "\n",
        "Problem- The model is not being trained properly as the loss is constant at 2 and the accuracy is not increasing and very poor.\n",
        "\n",
        "Fixing the problem- \n",
        "\n",
        "-Relu can be used instead of sigmoid activation function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNX5P7hQbrJO"
      },
      "source": [
        "Fail1.py\n",
        "\n",
        "Problem- nan Accuracy which is caused by vanishing gradient due to not proper weight updates of the inputs. So accuracy is very very less almost zero. It is a very dense neuralnetwork.\n",
        "\n",
        "Fixing problem-The weight initialization should be properly done. Instead of 8 layers number of layers is decreased to 2. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUVs_qTDjxRG"
      },
      "source": [
        "Fail3.py\n",
        "\n",
        "Problem- The loss is constant and non decreasing also the accuracy is very very small because the input weight is initialized by 0.\n",
        "\n",
        "Fix: The input weight can be initialized by 0.1 instead of 0.\n",
        "w_input = tf.Variable(tf.random.uniform([784, n_units], -w_range, 0.1),\n",
        "                      name=\"w0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOTQIYEa8HaX"
      },
      "source": [
        "Fail4.py\n",
        "\n",
        "Problem- Due to the standard deviation 2 in noise, the model is not able to detect image properly. So adding small amount of noise increases generelizatio not a lot.\n",
        "\n",
        "Fix- Making the standrad deviation to 0.1 instead of 2 increases the accuracy. \n",
        "\n",
        " img_batch += tf.random.normal(tf.shape(img_batch), stddev=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4you0tle89FA"
      },
      "source": [
        "Fail5.py\n",
        "\n",
        "Problem-In the model_forward(inputs) function in logits softmax is being used.If we use softmax in this layer all nodes (hidden variables) will be linearly dependent which may result in many problems and poor generalization.\n",
        "\n",
        "Fix- Not using softmax in that layer. Instead use-\n",
        "logits = tf.matmul(x, layers[-1][0] + layers[-1][1])"
      ]
    }
  ]
}